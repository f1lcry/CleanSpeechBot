# BotSummarizer

Telegram-бот для локальной обработки голосовых сообщений и выдачи отформатированного текста. Этот документ — основной хаб информации по проекту: архитектура, пайплайн, требования и ссылки на документацию.

## Глобальная идея

Бот принимает голосовые сообщения от пользователей, локально конвертирует и транскрибирует аудио, а затем передаёт полученный текст локальной модели Llama 3.1 8B (через Ollama) для очистки и лёгкой суммаризации. Весь пайплайн полностью офлайн: никакие внешние API не используются.

## Стек технологий

- **Язык:** Python 3.10+
- **Фреймворк бота:** Aiogram 3.x
- **Аудио-конвертация:** FFmpeg (OGG/MP3 → WAV)
- **Транскрибация:** openai-whisper (локальный inference)
- **LLM:** Ollama + Llama 3.1 8B (GGUF)
- **Асинхронность и очередь задач:** asyncio
- **Инфраструктура:** Docker, docker-compose, systemd (как fallback), Ubuntu 22.04 LTS, Timeweb Cloud (4 vCPU / 8 GB RAM / 40 GB SSD)

## Структура репозитория

```
project_root/
├── bot/
│   ├── main.py                # Точка входа для запуска бота
│   ├── config.py              # Конфигурация и чтение переменных окружения
│   ├── handlers/              # Aiogram-хэндлеры
│   │   ├── voice_handler.py   # Обработка голосовых сообщений
│   │   └── text_handler.py    # Доп. обработчики текстовых сообщений
│   ├── utils/                 # Вспомогательные модули
│   │   ├── audio.py           # Работа с аудио (конвертация, валидация)
│   │   ├── whisper_engine.py  # Обёртка над Whisper
│   │   ├── formatting_llm.py  # Взаимодействие с Ollama / Llama
│   │   └── workers.py         # Очереди задач и фоновые воркеры
│   └── services/
│       ├── pipeline.py        # Основной пайплайн обработки сообщений
│       └── logger.py          # Настройка логирования
├── models/
│   ├── whisper_cache/         # Кэш / артефакты для Whisper
│   └── llama_prompts/         # Промпты и шаблоны для LLM
├── deployments/
│   ├── Dockerfile             # Сборка контейнера бота
│   ├── docker-compose.yml     # Композиция сервисов (бот, Ollama, пр.)
│   └── systemd/
│       └── bot.service        # Юнит для systemd как альтернатива docker-compose
├── .env                       # Переменные окружения (не в git)
├── requirements.txt           # Python-зависимости
└── README.md                  # Этот файл
```

## Логика пайплайна

1. **Приём голосового сообщения.** Aiogram получает сообщение и сохраняет файл.
2. **Постановка в очередь.** Файл отправляется в очередь задач, чтобы ограничивать параллелизм и нагрузку на CPU.
3. **Обработка в `pipeline.py`:**
   - Конвертация входного аудио (OGG/MP3/WAV) в унифицированный WAV с помощью FFmpeg.
   - Транскрибация файла локальным Whisper (openai-whisper).
   - Форматирование, очистка и лёгкая суммаризация текста через локальную модель Llama 3.1 8B, поднятую в Ollama.
4. **Ответ пользователю.** Отформатированный текст отправляется обратно в чат.
5. **Уборка и логирование.** Временные файлы удаляются, ошибки фиксируются в логах и, при необходимости, триггерят fallback-сценарии.

## Функциональные требования к пайплайну

- Конвертация входного аудио (OGG/MP3/WAV) в единый формат для Whisper.
- Локальная транскрибация Whisper без внешних API.
- Запросы к Ollama API с локально развёрнутой Llama 3.1 8B.
- Очистка и структурирование текста: абзацы, пунктуация, удаление шумов.
- Очередь задач поверх asyncio для контроля нагрузки.
- Обработка ошибок, ретраи, fallback-ответы пользователю.
- Очистка временных файлов и контроль ограничений по длине аудио.
- Корректная работа с длинными сообщениями (разбиение, потоковая обработка, ограничения Telegram).

## Настройка окружения

1. **Системные требования**
   - Ubuntu 22.04 LTS
   - 4 vCPU / 8 GB RAM / 40 GB SSD (Timeweb Cloud)

2. **Предварительная установка**
   - Python 3.10+
   - FFmpeg
   - Docker и docker-compose
   - Ollama (для запуска Llama 3.1 8B)
   - Whisper модельные файлы (скачивание при первом запуске)

3. **Переменные окружения (`.env`)** — примерные ключи:

   ```env
   BOT_TOKEN=<telegram-bot-token>
   OLLAMA_HOST=http://localhost:11434
   WHISPER_MODEL=medium
   AUDIO_TMP_DIR=/tmp/botsummarizer
   TASK_QUEUE_LIMIT=2
   ```

4. **Установка зависимостей**

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

5. **Запуск в разработке**

   ```bash
   cd bot
   python main.py
   ```

6. **Запуск через Docker Compose**

   ```bash
   docker-compose up --build
   ```

## Очередь задач и производительность

- Очередь реализована в `bot/utils/workers.py`, управляется через asyncio.
- Ограничиваем количество параллельных транскрибаций и запросов к LLM, чтобы не перегружать CPU.
- Настройка размеров очереди и числа воркеров через переменные окружения / конфиг.
- План на будущее: метрики обработки, мониторинг времени выполнения и использование ресурсов.

## Логирование и мониторинг

- `bot/services/logger.py` — единая точка настройки логов (уровни, формат, вывод в файл/консоль).
- Логи ошибок и исключений позволяют анализировать сбои пайплайна.
- Возможные улучшения: интеграция с systemd journal, ротация логов, уведомления.

## Каталог `models/`

- `whisper_cache/` — кэшированные модели и данные Whisper для ускорения старта.
- `llama_prompts/` — промпты для форматирования текста, шаблоны ответов, системные инструкции.

## Развёртывание

- Основной сценарий: docker-compose с сервисами бота и Ollama.
- Альтернатива: systemd-юнит `deployments/systemd/bot.service` при запуске напрямую.
- Автоматизация обновлений: `git pull`, рестарт сервиса, миграция моделей.
- Бэкапы: периодическое копирование `models/` и логов.

## Документация и полезные ссылки

- [Aiogram 3.x](https://docs.aiogram.dev/en/latest/)
- [Whisper (openai-whisper)](https://github.com/openai/whisper)
- [FFmpeg](https://ffmpeg.org/documentation.html)
- [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Llama 3.1 (Meta)](https://huggingface.co/meta-llama)
- [Docker](https://docs.docker.com/)
- [docker-compose](https://docs.docker.com/compose/)
- [systemd](https://www.freedesktop.org/software/systemd/man/systemd.service.html)
- [Timeweb Cloud](https://timeweb.cloud/docs)

## TODO / идеи на будущее

- Добавить модуль уведомлений об ошибках (например, в отдельный чат админа).
- Реализовать прогресс обработки для пользователя (статусы «конвертация», «транскрибация», «форматирование»).
- Хранение истории запросов и результатов (для аналитики качества транскрибации и суммаризации).
- Тесты пайплайна и мокирование LLM/Whisper для unit-тестов.
- Настроить CI/CD (lint, тесты, сборка образов).

---

Любые дополнения по ходу разработки фиксируем здесь, чтобы Codex имел доступ ко всей необходимой информации для генерации кода.


## Статус реализации

- Подготовлен каркас проекта согласно описанной структуре.
- Конфигурация, логирование и основные сервисы созданы в виде заглушек с TODO.
