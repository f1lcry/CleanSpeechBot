# BotSummarizer

Telegram-бот для локальной обработки голосовых сообщений и выдачи отформатированного текста. Этот документ — основной хаб информации по проекту: архитектура, пайплайн, требования и ссылки на документацию.

## Глобальная идея

Бот принимает голосовые сообщения от пользователей, локально конвертирует и транскрибирует аудио, а затем передаёт полученный текст локальной модели Llama 3.1 8B (через Ollama) для очистки и лёгкой суммаризации. Весь пайплайн полностью офлайн: никакие внешние API не используются.

## Стек технологий

- **Язык:** Python 3.10+
- **Фреймворк бота:** Aiogram 3.x
- **Аудио-конвертация:** FFmpeg (OGG/MP3 → WAV)
- **Транскрибация:** openai-whisper (локальный inference)
- **LLM:** Ollama + Llama 3.1 8B (GGUF)
- **Асинхронность и очередь задач:** asyncio
- **Инфраструктура:** Docker, docker-compose, systemd (как fallback), Ubuntu 22.04 LTS, Timeweb Cloud (4 vCPU / 8 GB RAM / 40 GB SSD)

## Структура репозитория

```
project_root/
├── bot/
│   ├── main.py                # Точка входа для запуска бота
│   ├── config.py              # Конфигурация и чтение переменных окружения
│   ├── handlers/              # Aiogram-хэндлеры
│   │   ├── voice_handler.py   # Обработка голосовых сообщений
│   │   └── text_handler.py    # Доп. обработчики текстовых сообщений
│   ├── utils/                 # Вспомогательные модули
│   │   ├── audio.py           # Работа с аудио (конвертация, валидация)
│   │   ├── whisper_engine.py  # Обёртка над Whisper
│   │   ├── formatting_llm.py  # Взаимодействие с Ollama / Llama
│   │   └── workers.py         # Очереди задач и фоновые воркеры
│   └── services/
│       ├── pipeline.py        # Основной пайплайн обработки сообщений
│       └── logger.py          # Настройка логирования
├── models/
│   ├── whisper_cache/         # Кэш / артефакты для Whisper
│   └── llama_prompts/         # Промпты и шаблоны для LLM
├── deployments/
│   ├── Dockerfile             # Сборка контейнера бота
│   ├── docker-compose.yml     # Композиция сервисов (бот, Ollama, пр.)
│   └── systemd/
│       └── bot.service        # Юнит для systemd как альтернатива docker-compose
├── .env                       # Переменные окружения (не в git)
├── requirements.txt           # Python-зависимости
└── README.md                  # Этот файл
```

## Логика пайплайна

1. **Приём голосового сообщения.** Aiogram получает сообщение и сохраняет файл.
2. **Постановка в очередь.** Файл отправляется в очередь задач, чтобы ограничивать параллелизм и нагрузку на CPU.
3. **Обработка в `pipeline.py`:**
   - Конвертация входного аудио (OGG/MP3/WAV) в унифицированный WAV с помощью FFmpeg.
   - Транскрибация файла локальным Whisper (openai-whisper).
   - Форматирование, очистка и лёгкая суммаризация текста через локальную модель Llama 3.1 8B, поднятую в Ollama.
4. **Ответ пользователю.** Отформатированный текст отправляется обратно в чат.
5. **Уборка и логирование.** Временные файлы удаляются, ошибки фиксируются в логах и, при необходимости, триггерят fallback-сценарии.

## Безопасность, приватность и Stream Processing

Проект строится вокруг принципа «stream processing»: данные проходят через сервис конвейером (вход → обработка → ответ → уничтожение) и не хранятся дольше, чем требуется для конкретной задачи. Политика прозрачности подчёркивает, что бот работает **исключительно на локальных моделях** (Whisper + Llama 3.1 8B через Ollama), не отправляет данные во внешние API и не сохраняет пользовательские аудио, транскрипты или историю переписки.

- **Аудио:**
  - Загруженные файлы сохраняются только во временных локациях (RAM или `/tmp`).
  - После конвертации и транскрибации любой файл немедленно удаляется; использование постоянных томов или бэкапов с пользовательским контентом запрещено.
- **Транскрипты:**
  - Текст живёт только в оперативной памяти в рамках задачи.
  - После отправки ответа пользователю текст полностью уничтожается и не попадает ни в файлы, ни в БД, ни в логи.
- **База данных:**
  - Хранит только технические метаданные: `chat_id` (желательно захешированный с солью), анонимизированный `user_id`, настройки чата (пороги длительности, режимы обработки), счётчики запросов для будущей монетизации.
  - История сообщений, аудио, транскрипты и любой пользовательский контент запрещены к сохранению.
- **Кэш:**
  - Кэширование аудио и транскриптов отсутствует; допускаются только временные структуры данных в памяти (например, `asyncio.Queue`).
  - Если нужен кратковременный кэш ради оптимизации, он ограничен временем жизни процесса и никогда не записывается на диск.
- **Логирование:**
  - Логи содержат исключительно технические события (`voice_received`, `transcription_success`, `llm_error` и т. п.), UUID задачи, длительность обработки, размеры файлов и тайминги этапов.
  - Текст пользовательских сообщений, имена, raw JSON обновлений Telegram и любые транскрипты логировать запрещено.
- **Прозрачность и доверие:**
  - README фиксирует, что после обработки на диске не остаётся следов данных, а вся обработка выполняется офлайн локальными моделями.
  - Ротация логов возможна стандартными средствами, но без сохранения пользовательского контента.

## Функциональные требования к пайплайну

- Конвертация входного аудио (OGG/MP3/WAV) в единый формат для Whisper.
- Локальная транскрибация Whisper без внешних API.
- Запросы к Ollama API с локально развёрнутой Llama 3.1 8B.
- Очистка и структурирование текста: абзацы, пунктуация, удаление шумов.
- Очередь задач поверх asyncio для контроля нагрузки.
- Обработка ошибок, ретраи, fallback-ответы пользователю.
- Очистка временных файлов и контроль ограничений по длине аудио.
- Корректная работа с длинными сообщениями (разбиение, потоковая обработка, ограничения Telegram).

## Настройка окружения

1. **Системные требования**
   - Ubuntu 22.04 LTS
   - 4 vCPU / 8 GB RAM / 40 GB SSD (Timeweb Cloud)

2. **Предварительная установка**
   - Python 3.10+
   - FFmpeg (должен быть доступен в `$PATH`)
   - GPU-драйверы/CUDA Toolkit, если планируется использовать `--device cuda` в Whisper
   - Docker и docker-compose
   - Ollama (для запуска Llama 3.1 8B)
   - Whisper модельные файлы (скачивание при первом запуске)

3. **Переменные окружения (`.env`)** — примерные ключи:

   ```env
   BOT_TOKEN=<telegram-bot-token>
   OLLAMA_HOST=http://localhost:11434
   WHISPER_MODEL=medium
   AUDIO_TMP_DIR=/tmp/botsummarizer
   TASK_QUEUE_LIMIT=2
   ```

4. **Установка зависимостей**

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

   `requirements.txt` тянет все необходимые Python-библиотеки: `aiogram`, `python-dotenv`, `ollama`, а также аудио-стек (`openai-whisper`, `torch==2.1.2`, `numpy<2`). Whisper подгружается локально — отдельного шага установки не требуется, модель скачивается автоматически в директорию кэша при первом запуске.

5. **Запуск в разработке**

   ```bash
   cd bot
   python main.py
   ```

6. **Запуск через Docker Compose**

   ```bash
   docker-compose up --build
   ```

## Модуль конвертации

Для локальной отладки конвертации добавлен отдельный CLI-скрипт `tools/convert_audio.py`, который использует те же правила, что и бот. Системные требования прежние: Python 3.10+ и установленный в системе FFmpeg (должен быть доступен в `$PATH`).

- **Аргументы**
  - `--input` (обязательный): относительный или абсолютный путь к исходному `.ogg` (или другому поддерживаемому FFmpeg) файлу.
  - `--output-dir` (опциональный): каталог назначения. По умолчанию берётся `AUDIO_TMP_DIR` (если переменная окружения отсутствует, используется `/tmp/botsummarizer`).
  - `--output-name` (опциональный): имя выходного файла без расширения. Если не задано, используется `stem` исходного файла и авто-суффикс.
  - `--validate`: включает проверку, что WAV соответствует требованиям Whisper (16 кГц, mono, PCM S16LE).

- **Дефолтные пути**. Все временные WAV складываются в каталог из `AUDIO_TMP_DIR`. CLI автоматически создаёт каталог, если он отсутствует, и удаляет файлы по `AudioProcessor.cleanup()` в боевом пайплайне.

- **Пример запуска** (добавляйте эту команду в summary Codex, если правите конвертер):

  ```bash
  python tools/convert_audio.py --input test_files/voice_message.ogg --validate
  ```

  Ожидаемый вывод:

  ```
  [INFO] Converted file saved to: /tmp/botsummarizer/voice_message_xxxxx.wav
  ```

  Проверить результат можно через:

  ```bash
  ls /tmp/botsummarizer
  file /tmp/botsummarizer/voice_message_*.wav
  ```

  При любых ошибках (нет входного файла, FFmpeg не установлен, невалидный формат) CLI завершится с ненулевым кодом и напечатает `[ERROR] Conversion failed: ...`.

## Модуль транскрибации

Для локальной отладки транскрибации добавлен CLI `tools/transcribe_audio.py`. Он использует `WhisperEngine` и ожидает наличие WAV в каталоге `AUDIO_TMP_DIR/Temporary` (туда же по умолчанию пишет `tools/convert_audio.py`). Если `--input` не передан, скрипт выбирает последний по времени изменения `.wav` в целевой директории.

- **Базовый запуск** (поиск последнего файла в `Temporary`):

  ```bash
  python tools/transcribe_audio.py
  ```

  Ожидаемый вывод:

  ```
  [INFO] Transcribing voice_message.wav (...)
  [TRANSCRIPT] <итоговый текст>
  ```

- **Запуск с явным путём и параметрами модели**:

  ```bash
  python tools/transcribe_audio.py --input /tmp/botsummarizer/Temporary/voice.wav --model medium --language ru
  ```

  Вывод аналогичен: в stdout появится строка `[TRANSCRIPT] ...`, а логирование расскажет о загрузке модели, длительности аудио и выбранных параметрах. Ошибки (отсутствует файл, нет WAV, проблемы с моделью) приводят к сообщению `[ERROR] Transcription failed: ...` и завершению с кодом `1`.

Перед запуском убедитесь, что WAV-файл уже подготовлен (через `tools/convert_audio.py` или другой совместимый способ) и лежит в директории `Temporary`.

## Очередь задач и производительность

- Очередь реализована в `bot/utils/workers.py`, управляется через asyncio.
- Ограничиваем количество параллельных транскрибаций и запросов к LLM, чтобы не перегружать CPU.
- Настройка размеров очереди и числа воркеров через переменные окружения / конфиг.
- План на будущее: метрики обработки, мониторинг времени выполнения и использование ресурсов.

## Логирование и мониторинг

- `bot/services/logger.py` — единая точка настройки логов (уровни, формат, вывод в файл/консоль).
- Логи ошибок и исключений позволяют анализировать сбои пайплайна.
- Возможные улучшения: интеграция с systemd journal, ротация логов, уведомления.

## Каталог `models/`

- `whisper_cache/` — кэшированные модели и данные Whisper для ускорения старта.
- `llama_prompts/` — промпты для форматирования текста, шаблоны ответов, системные инструкции.

## Развёртывание

- Основной сценарий: docker-compose с сервисами бота и Ollama.
- Альтернатива: systemd-юнит `deployments/systemd/bot.service` при запуске напрямую.
- Автоматизация обновлений: `git pull`, рестарт сервиса, миграция моделей.
- Бэкапы: периодическое копирование `models/` и логов.

## Документация и полезные ссылки

- [Aiogram 3.x](https://docs.aiogram.dev/en/latest/)
- [Whisper (openai-whisper)](https://github.com/openai/whisper)
- [FFmpeg](https://ffmpeg.org/documentation.html)
- [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Llama 3.1 (Meta)](https://huggingface.co/meta-llama)
- [Docker](https://docs.docker.com/)
- [docker-compose](https://docs.docker.com/compose/)
- [systemd](https://www.freedesktop.org/software/systemd/man/systemd.service.html)
- [Timeweb Cloud](https://timeweb.cloud/docs)

## TODO / идеи на будущее

- Добавить модуль уведомлений об ошибках (например, в отдельный чат админа).
- Реализовать прогресс обработки для пользователя (статусы «конвертация», «транскрибация», «форматирование»).
- Добавить расширенные метрики по техническим событиям (время этапов, счётчики задач) без хранения пользовательского контента.
- Тесты пайплайна и мокирование LLM/Whisper для unit-тестов.
- Настроить CI/CD (lint, тесты, сборка образов).

---

Любые дополнения по ходу разработки фиксируем здесь, чтобы Codex имел доступ ко всей необходимой информации для генерации кода.


## Статус реализации

- Подготовлен каркас проекта согласно описанной структуре.
- Конфигурация, логирование и основные сервисы созданы в виде заглушек с TODO.
