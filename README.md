# BotSummarizer

Telegram-бот для локальной обработки голосовых сообщений и выдачи отформатированного текста. Этот документ — основной хаб информации по проекту: архитектура, пайплайн, требования и ссылки на документацию.

## Глобальная идея

Бот принимает голосовые сообщения от пользователей, локально конвертирует и транскрибирует аудио, а затем передаёт полученный текст локальной модели Llama 3.1 8B (через Ollama) для очистки и лёгкой суммаризации. Весь пайплайн полностью офлайн: никакие внешние API не используются.

## Стек технологий

- **Язык:** Python 3.10+
- **Фреймворк бота:** Aiogram 3.x
- **Аудио-конвертация:** FFmpeg (OGG/MP3 → WAV)
- **Транскрибация:** openai-whisper (локальный inference)
- **LLM:** Ollama + Llama 3.1 8B (GGUF)
- **Асинхронность и очередь задач:** asyncio
- **Инфраструктура:** Docker, docker-compose, systemd (как fallback), Ubuntu 22.04 LTS, Timeweb Cloud (4 vCPU / 8 GB RAM / 40 GB SSD)

## Структура репозитория

```
project_root/
├── bot/
│   ├── main.py                # Точка входа для запуска бота
│   ├── config.py              # Конфигурация и чтение переменных окружения
│   ├── handlers/              # Aiogram-хэндлеры
│   │   ├── voice_handler.py   # Обработка голосовых сообщений
│   │   └── text_handler.py    # Доп. обработчики текстовых сообщений
│   ├── utils/                 # Вспомогательные модули
│   │   ├── audio.py           # Работа с аудио (конвертация, валидация)
│   │   ├── whisper_engine.py  # Обёртка над Whisper
│   │   ├── formatting_llm.py  # Взаимодействие с Ollama / Llama
│   │   └── workers.py         # Очереди задач и фоновые воркеры
│   └── services/
│       ├── pipeline.py        # Основной пайплайн обработки сообщений
│       └── logger.py          # Настройка логирования
├── models/
│   ├── whisper_cache/         # Кэш / артефакты для Whisper
│   └── llama_prompts/         # Промпты и шаблоны для LLM
├── deployments/
│   ├── Dockerfile             # Сборка контейнера бота
│   ├── docker-compose.yml     # Композиция сервисов (бот, Ollama, пр.)
│   └── systemd/
│       └── bot.service        # Юнит для systemd как альтернатива docker-compose
├── .env                       # Переменные окружения (не в git)
├── requirements.txt           # Python-зависимости
└── README.md                  # Этот файл
```

## Логика пайплайна

1. **Приём голосового сообщения.** Aiogram получает сообщение и сохраняет файл.
2. **Постановка в очередь.** Файл отправляется в очередь задач, чтобы ограничивать параллелизм и нагрузку на CPU.
3. **Обработка в `pipeline.py`:**
   - Конвертация входного аудио (OGG/MP3/WAV) в унифицированный WAV с помощью FFmpeg.
   - Транскрибация файла локальным Whisper (openai-whisper).
   - Форматирование, очистка и лёгкая суммаризация текста через локальную модель Llama 3.1 8B, поднятую в Ollama.
4. **Ответ пользователю.** Отформатированный текст отправляется обратно в чат.
5. **Уборка и логирование.** Временные файлы удаляются, ошибки фиксируются в логах и, при необходимости, триггерят fallback-сценарии.

## Безопасность, приватность и Stream Processing

Проект строится вокруг принципа «stream processing»: данные проходят через сервис конвейером (вход → обработка → ответ → уничтожение) и не хранятся дольше, чем требуется для конкретной задачи. Политика прозрачности подчёркивает, что бот работает **исключительно на локальных моделях** (Whisper + Llama 3.1 8B через Ollama), не отправляет данные во внешние API и не сохраняет пользовательские аудио, транскрипты или историю переписки.

- **Аудио:**
  - Загруженные файлы сохраняются только во временных локациях (RAM или `/tmp`).
  - После конвертации и транскрибации любой файл немедленно удаляется; использование постоянных томов или бэкапов с пользовательским контентом запрещено.
- **Транскрипты:**
  - Текст живёт только в оперативной памяти в рамках задачи.
  - После отправки ответа пользователю текст полностью уничтожается и не попадает ни в файлы, ни в БД, ни в логи.
- **База данных:**
  - Хранит только технические метаданные: `chat_id` (желательно захешированный с солью), анонимизированный `user_id`, настройки чата (пороги длительности, режимы обработки), счётчики запросов для будущей монетизации.
  - История сообщений, аудио, транскрипты и любой пользовательский контент запрещены к сохранению.
- **Кэш:**
  - Кэширование аудио и транскриптов отсутствует; допускаются только временные структуры данных в памяти (например, `asyncio.Queue`).
  - Если нужен кратковременный кэш ради оптимизации, он ограничен временем жизни процесса и никогда не записывается на диск.
- **Логирование:**
  - Логи содержат исключительно технические события (`voice_received`, `transcription_success`, `llm_error` и т. п.), UUID задачи, длительность обработки, размеры файлов и тайминги этапов.
  - Текст пользовательских сообщений, имена, raw JSON обновлений Telegram и любые транскрипты логировать запрещено.
- **Прозрачность и доверие:**
  - README фиксирует, что после обработки на диске не остаётся следов данных, а вся обработка выполняется офлайн локальными моделями.
  - Ротация логов возможна стандартными средствами, но без сохранения пользовательского контента.

## Функциональные требования к пайплайну

- Конвертация входного аудио (OGG/MP3/WAV) в единый формат для Whisper.
- Локальная транскрибация Whisper без внешних API.
- Запросы к Ollama API с локально развёрнутой Llama 3.1 8B.
- Очистка и структурирование текста: абзацы, пунктуация, удаление шумов.
- Очередь задач поверх asyncio для контроля нагрузки.
- Обработка ошибок, ретраи, fallback-ответы пользователю.
- Очистка временных файлов и контроль ограничений по длине аудио.
- Корректная работа с длинными сообщениями (разбиение, потоковая обработка, ограничения Telegram).

## Настройка окружения

1. **Системные требования**
   - Ubuntu 22.04 LTS
   - 4 vCPU / 8 GB RAM / 40 GB SSD (Timeweb Cloud)

2. **Предварительная установка**
   - Python 3.10+
   - FFmpeg (должен быть доступен в `$PATH`)
   - GPU-драйверы/CUDA Toolkit, если планируется использовать `--device cuda` в Whisper
   - Docker и docker-compose
   - Ollama (для запуска Llama 3.1 8B)
   - Whisper модельные файлы (скачивание при первом запуске)

3. **Переменные окружения (`.env`)** — примерные ключи:

   ```env
   BOT_TOKEN=<telegram-bot-token>
   OLLAMA_HOST=http://localhost:11434
   # Настройка форматирования/LLM
   FORMATTER_MODEL=llama3.1:8b
   FORMATTER_TIMEOUT=120
   FORMATTER_PROMPT_SOURCE=file
   FORMATTER_PROMPT_PATH=models/llama_prompts/formatter_system_prompt.txt
   # FORMATTER_PROMPT_TEXT=""  # Используется, если FORMATTER_PROMPT_SOURCE=inline

   # Whisper и аудио
   WHISPER_MODEL=medium
   WHISPER_LANGUAGE=ru
   WHISPER_TEMPERATURE=0.0
   WHISPER_DEVICE=cpu
   WHISPER_CACHE_DIR=./models/whisper_cache
   WHISPER_CA_BUNDLE=/etc/ssl/certs/corporate-ca.pem
   WHISPER_INSECURE_SSL=false
   AUDIO_TMP_DIR=/tmp/botsummarizer
   TASK_QUEUE_LIMIT=2
   ```

4. **Установка зависимостей**

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

   `requirements.txt` тянет все необходимые Python-библиотеки: `aiogram`, `python-dotenv`, `ollama`, а также аудио-стек (`openai-whisper==20250625`, `torch==2.1.2`, `numpy<2`). Whisper подгружается локально — отдельного шага установки не требуется, модель скачивается автоматически в директорию кэша при первом запуске. Если корпоративная сеть подменяет SSL-сертификаты, укажите путь к корневому сертификату через `WHISPER_CA_BUNDLE` (или CLI-параметр `--ca-bundle`). В крайнем случае доступно отключение проверки (`WHISPER_INSECURE_SSL=true` или `--insecure-ssl`).

5. **Запуск в разработке**

   ```bash
   cd bot
   python main.py
   ```

6. **Запуск через Docker Compose**

   ```bash
   docker-compose up --build
   ```

## Модуль конвертации

Для локальной отладки конвертации добавлен отдельный CLI-скрипт `tools/convert_audio.py`, который использует те же правила, что и бот. Все CLI автоматически подхватывают `.env` через `tools/_env.py`, поэтому можно задавать переменные один раз.

- **Системные требования**: Python 3.10+, установленный FFmpeg, доступный в `$PATH`.
- **Аргументы CLI**:
  - `--input` — путь к исходному `.ogg`/`.mp3` (обязательный).
  - `--output-dir` — каталог назначения (по умолчанию `AUDIO_TMP_DIR`).
  - `--output-name` — имя выходного файла без расширения.
  - `--validate` — выполнить проверку параметров WAV после конвертации.
- **Переменные окружения**:
  - `AUDIO_TMP_DIR` — базовый каталог для временных WAV.
- **Пример**:

  ```bash
  python tools/convert_audio.py --input test_files/voice_message.ogg --validate
  ```

  После успешного запуска в логе появится `Converted file saved to: <путь>` и файл можно проверить командами `ls`/`file`.

## Модуль транскрибации

CLI `tools/transcribe_audio.py` оборачивает `WhisperEngine` и использует те же настройки, что и продакшен.

- **Аргументы CLI**:
  - `--input` — путь к WAV. Если не указан, берётся последний файл в `--tmp-dir`.
  - `--tmp-dir` — где лежат WAV (по умолчанию `AUDIO_TMP_DIR`).
  - `--model`, `--language`, `--temperature`, `--device` — пробрасываются в Whisper.
  - `--ca-bundle`, `--insecure-ssl` — настройки TLS для скачивания модели.
  - `--verbose` — расширенное логирование.
- **Переменные окружения**:
  - `AUDIO_TMP_DIR`, `WHISPER_MODEL`, `WHISPER_LANGUAGE`, `WHISPER_DEVICE`, `WHISPER_CACHE_DIR`, `WHISPER_CA_BUNDLE`, `WHISPER_INSECURE_SSL`.
- **Примеры**:

  ```bash
  python tools/transcribe_audio.py
  python tools/transcribe_audio.py --input /tmp/botsummarizer/voice.wav --language ru --device cpu
  ```

  В stdout всегда приходит строка `[TRANSCRIPT] ...`. Ошибки приводят к `[ERROR] Transcription failed: ...` и коду выхода `1`.

Если при скачивании модели возникает `SSL: CERTIFICATE_VERIFY_FAILED`, воспользуйтесь параметром `--ca-bundle` или `WHISPER_CA_BUNDLE`. Флаг `--insecure-ssl` (или `WHISPER_INSECURE_SSL=true`) отключает проверку, но используйте его только как временный обходной путь.

## Модуль форматирования

`tools/format_text.py` позволяет локально проверить работу `FormattingLLMClient` без запуска Telegram-бота.

- **Требования**: запущенный Ollama с нужной моделью и доступ к файлу системного промпта.
- **Аргументы CLI**:
  - Источник транскрипта: `--text`, `--input-file` или stdin.
  - `--host`, `--model` — переопределяют `OLLAMA_HOST`/`FORMATTER_MODEL`.
  - `--prompt`/`--prompt-file` — временно подменяют системный промпт.
  - `--timeout` — таймаут запроса в секунду (по умолчанию `FORMATTER_TIMEOUT`).
  - `--verbose` — подробные логи и прогресс стриминга.
- **Переменные окружения**:
  - `OLLAMA_HOST`, `FORMATTER_MODEL`, `FORMATTER_TIMEOUT`, `FORMATTER_PROMPT_SOURCE`, `FORMATTER_PROMPT_PATH`, `FORMATTER_PROMPT_TEXT`.
- **Пример**:

  ```bash
  python tools/format_text.py --text "привет это тест" --verbose
  python tools/format_text.py --input-file transcript.txt --prompt-file models/llama_prompts/formatter_system_prompt.txt
  ```

CLI выводит только отформатированный текст. При ошибке взаимодействия с Ollama команда вернёт код `1` и сообщение вида `Formatting failed: ...`.

## Очередь задач и производительность

- Очередь реализована в `bot/utils/workers.py`, управляется через asyncio.
- Ограничиваем количество параллельных транскрибаций и запросов к LLM, чтобы не перегружать CPU.
- Настройка размеров очереди и числа воркеров через переменные окружения / конфиг.
- План на будущее: метрики обработки, мониторинг времени выполнения и использование ресурсов.

## Логирование и мониторинг

- `bot/services/logger.py` — единая точка настройки логов (уровни, формат, вывод в файл/консоль).
- Логи ошибок и исключений позволяют анализировать сбои пайплайна.
- Возможные улучшения: интеграция с systemd journal, ротация логов, уведомления.

## Каталог `models/`

- `whisper_cache/` — кэшированные модели и данные Whisper для ускорения старта.
- `llama_prompts/` — промпты для форматирования текста, шаблоны ответов, системные инструкции.

## Развёртывание

- Основной сценарий: docker-compose с сервисами бота и Ollama.
- Альтернатива: systemd-юнит `deployments/systemd/bot.service` при запуске напрямую.
- Автоматизация обновлений: `git pull`, рестарт сервиса, миграция моделей.
- Бэкапы: периодическое копирование `models/` и логов.

## Документация и полезные ссылки

- [Aiogram 3.x](https://docs.aiogram.dev/en/latest/)
- [Whisper (openai-whisper)](https://github.com/openai/whisper)
- [FFmpeg](https://ffmpeg.org/documentation.html)
- [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Llama 3.1 (Meta)](https://huggingface.co/meta-llama)
- [Docker](https://docs.docker.com/)
- [docker-compose](https://docs.docker.com/compose/)
- [systemd](https://www.freedesktop.org/software/systemd/man/systemd.service.html)
- [Timeweb Cloud](https://timeweb.cloud/docs)

## TODO / идеи на будущее

- Добавить модуль уведомлений об ошибках (например, в отдельный чат админа).
- Реализовать прогресс обработки для пользователя (статусы «конвертация», «транскрибация», «форматирование»).
- Добавить расширенные метрики по техническим событиям (время этапов, счётчики задач) без хранения пользовательского контента.
- Тесты пайплайна и мокирование LLM/Whisper для unit-тестов.
- Настроить CI/CD (lint, тесты, сборка образов).

---

Любые дополнения по ходу разработки фиксируем здесь, чтобы Codex имел доступ ко всей необходимой информации для генерации кода.


## Статус реализации

- Подготовлен каркас проекта согласно описанной структуре.
- Конфигурация, логирование и основные сервисы созданы в виде заглушек с TODO.
